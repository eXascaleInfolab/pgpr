review
SUMMARY The paper is concerned with the problem of arbitrary feed-forward style transfer, where a feed-forward model receives a content image and a style image as input, and must produce as output an image matching the content of the former and the style of the latter. The approach roughly follows that of [Li et al, NIPS 2017]: An encoder network (VGG pretrained on ImageNet) is used to extract features from both the style and content image; the features from the content image are adjusted to match the statistics from the style image, and the adjusted features are passed to a decoder network which generates the output image.  Compared to prior work, the main innovations are: - Considering correlations between features from different encoder layers rather than only correlations within a single layer in both the feature adjustment step as well as in the loss - An improved encoder / decoder architecture which uses skip connections in the decoder, allowing for a single encoder / decoder pair rather than a cascade of encoder / decoder pairs for each layer.  PROS - Considering correlations between features from different encoder layers is a good idea - The improved encoder / decoder architecture is significantly more efficient than the cascaded approach of [Li et al, NIPS 2017]  CONS - Somewhat incremental - Limited experimental evaluation - Qualitative results not clearly better than existing methods - Missing citation for multi-scale losses  LIMITED EXPERIMENTAL EVALUATION One of the key claims of the paper is that “our method with inter-scale (fig.7(f)) or intra-scale feature transform (fig.7(g)) are more similar to the target style than those of single-scale style transfer without considering inter-channel correlation” (Figure 7 caption); this claim is substantiated primarily by qualitative results in Figures 4, 7, and 8. Personally I don’t find the results with inter-feature correlations to be much better than those with only intra-feature correlations or the results from prior work. All recent style transfer methods depend on a host of hyperparameters like style and content weight, learning schedule, etc; in my experience differences in these hyperparameter settings can have large effects on the qualitative appearance of the generated images, and by varying these hyperparameters it is common to see qualitative differences similar to those shown in Figure 4 and 5. From the small number of qualitative results presented I do not think that the benefits of inter-scale correlations have been clearly demonstrated.   I appreciate that the authors tried to quantify their results by comparing loss values (Table 1) but unfortunately it’s hard to know how much the values of different losses correlate with human judgement of style quality.  In addition to selected qualitative results I would have liked to see a user study demonstrating human preference for images generated using inter-scale correlation losses, ideally across a range of different hyperparameter settings for each method.  GATYS BASELINE From Table 1, the proposed method with intra-scale features achieves lower content loss than the direct optimization baseline (a) from [Gatys et al.] This is very surprising to me - typically direct optimization leads to much lower losses than any feedforward methods. From Section 4.4.1 this baseline uses Adam; in my experience using L-BFGS tends to achieve lower losses which may explain the results from Table 1.  MISSING CITATION FOR MULTI-SCALE STYLE TRANSFER Instead of computing correlations between features from different encoder layers, [Wang et al, CVPR 2017] define a loss that considers the generated and style image at multiple spatial scales. This should be discussed in relation to the proposed method.  ENCODER / DECODER DETAILS There are some missing details about exactly how the encoder and decoder are initialized and trained. I assume that the encoder was pretrained on ImageNet; is it updated during training or kept fixed? Is the decoder initialized randomly or does it mirror the pretrained ImageNet weights?  TYPOS / FORMATTING There are minor typos throughout, e.g. “verity”, “sinlge” in Section 4.3, Paragrah 1. I also found the citation style to be somewhat jarring, especially in the introduction; parenthetical citations and better spacing may improve readability.  OVERALL On the whole I feel that the paper is somewhat incremental. The inter-scale loss seems intuitively like a good idea, but I don’t think the paper presents sufficient experimental evidence to justify it. On the other hand the proposed decoder architecture seems like a clear improvement over the cascaded approach from [Li et al, NIPS 2017], as it is significantly more efficient without sacrificing quality. However I don’t feel that this alone is enough novelty for ICLR, so I lean slightly toward rejection.
This work proposes to use a single feed-forward network with two types of multi-scale transformers (MST) for image style transfer. The first transformer cascades existing single-scale transforms (SSTs), and the second one applies SST to the stacked feature maps. Skip connection is used between the MSTs and the decoder.  Pros  - Has quantitative evaluation Table 1 includes quantitative results for different approaches, which is essential for proper evaluation.  Cons  - The problem is not very well motivated and the novelty is limited.  Why shall we care about multi-scale style transformation? Style transformation is about modifying an image to match certain style WITHOUT completely destroy its content. Allowing low level details to interfere high level content seems to be a bad idea. As for novelty, it is claimed that this work is the first to use skp connection. However, Avatar-Net uses skip connection before.  - Empirical results not significant In Table 1, why is (d) missing for the small set and (b) missing for the large set? It seems that (a) and (b) are already very good. Then why do we need the proposed methods? It is said that (b) is not extendable to arbitrary style transfer. How so? How do you define "arbitrary style transferring methods"? I do not see how Fig.6 can tell us useful information about the skip connections. The corresponding paragraph in Sec.4.3 is not very convincing or informative.  - Writing can be improved. In terms of content, terminologies are used without clear definitions. For example, what is "inter-scale dependency" in the introduction and what does "merges multi-scaled styles optimally" mean in Sec.3.1.3 (what is the optimality here)? It is confusing what corresponds to "intra-scale" or "inter-scale" throughout the paper. For example, the direct connection between relu_3_3 to relu_2_2 of the decoder in Fig.1b can also be interpreted as "inter-scale". As another example, "4 batches of random image pairs" in the experiment, do you mean a batch of 4 random pairs? In terms of presentation, the grammar needs more careful checking. For examples, "in the remained of this paper", "each methods", etc. The meaning of the transpose in Eq.(3) is not clear. How do we transpose 3-dimensional tensor F? Also, know the difference between \citet and \citep and when to use them.  Minors - In Eq.(4), according to the definition of C_i, for i > 1, the index should be (\sum_k C_k + 1):(\sum_k C_k + C_i) - Above Eq.(5), "inter-scale feature transform (Sec.3.1.1)" should be Sec.3.1.2. - In Fig. 7, the last column should be (g) instead of (e).
The paper proposed a NN-based model for open set recognition via finding a better feature space where larger inter-class (P2) and smaller intra-class distances (P1) are satisfied. In the proposed model, the inter- and intra-class distances are measured basing on the mean of final linear layer features from each class, and a kind of L2 loss is defined to ensure the properties of the feature space during the training progress. Then the proposed outlier score defined as the minimum inter-class distance becomes the key for the open set recognition task.  Generally, this paper is well written and easy to read. The proposed threshold estimation method for outlier score based on assumed contamination ratio is reasonable. And three datasets in two domains are used to prove the model’s effectiveness.  Major comments: 1. This paper seems less novel. There exist several methods aiming to find a better feature space satisfying the mentioned feature distance properties by adjusting the optimized loss functions, such as Center Loss (Wen et.al 2016) and Additive Angular Margin (Deng et.al 2018). I think the idea Combining ii-loss with Cross Entropy Loss proposed in this paper is quite similar to the Center Loss except that a) the ii-loss contains a part for maximizing the minimum inter-class distance; b) the ii-loss and cross entropy loss are optimized separately. Since results shown in Center Loss that without pushing the inter-class distance, the feature space still satisfied P1 and P2, this paper seems not that novel, at least some comparation can be added to analyze the improvement for adding the inter-class part.  2. This paper seems less convincing as well. The paper introduces that the two properties (larger inter-class and smaller intra-class distances) can lead to larger spaces among known classes for the instances of the unknown classes to occupy. However, it keeps uncertain if this can be generalized to unseen classes. In this sense, it is better to conduct some additional theoretical analysis or perform more experiments to validate this. In particular, only one plot was performed to verify this point on one single dataset. Maybe more plots of the distributions can be provided on more additional datasets.
The paper focuses on open set classification where one wants to design a classifier able to accurately classify samples from training (known classes) and able to reject samples from unknown classes. Such a feature is would be clearly desirable for all machine learning classification algorithms. The paper presents a representation learning based approach for this problem.   The idea is very simple. It consists in learning a neural classifier with a constraint on the representation space of samples (i.e. the one implemented in a chosen hidden layer of the network) aiming at optimizing a Fisher-discriminant-like criterion. This criterion aims at minimizing the variance of the representation of the samples within a class and to make representations of samples from different classes (actually the means per class) well separated. The learning is eventually performed by adding a usual cross entropy classification loss on the output layer to the Fisher like criterion. The rejection of samples from unknown classes is performed via a threshold on the minimum distance of a sample representation and the class means in the representations space.  The idea is well thought but the innovation is indeed low.   Experiments are performed on three, but small, datasets including the simple Mnist dataset. Experimental results compare the proposed approach and its variants to two recent baselines, OpenMax and G_OpenMax. Experiments show the proposed approach outperform the two baselines but in some cases the confidence interval is quite large and prevent definitive conclusions (e.g. up to 0.05 in Table 2). Visualization of projected data show as expected the interesting feature of the representation space. Yet the experimental analysis does not seem as deeps the ones in the two papers where baselines were published. For instance results are shown with respect to a measure of the experimental setting named openness in [Ge and al.]. Moreover the paper by  [Ge and al.] conclude to the superiority of their proposal with respect to OpenMax which is not fully consistent with the results reported in this paper. Also experiments were performed on much bigger datasets in these two references with ILSVRC 2012 dataset in [Bendale and al., 2016] while [Ge and al.]  used a handwritten diet dataset with more than 350 classes. It would drastically strengthen the paper if the authors could provide comparative results on these datasets too.
This paper deals with the open set classification problem, where in addition to the known classes, the method should also be able to recognize the unknown class. The main idea is based on two parts: learning a discriminative representation, and a threshold based detection rule. To learn the embedding, the authors propose to minimize the inner class distance (between each instance to its center) and enlarge the distance between centers. The outlier score of an instance is computed as the minimum distance between known class prototypes. Experiments on various datasets show the ability of the learned method.  I'm not completely sure whether the whole approach is novel or not in the open set recognition domain, but both parts are not novel enough. Pulling similar instances together and pushing dissimilar ones away is the main idea in embedding learning. The ii-loss is similar to the triplet-center loss in the paper "He et al. Triplet-Center Loss for Multi-View 3D Object Retrieval. CVPR18".   Although in the experiments the proposed method achieves good results in most cases, the reviewer suggests the authors comparing with more baselines to make the work solid. 1. Comparing with other embedding learning methods with the same outlier detection score.  The authors should prove that the proposed embedding is important enough in the open set case. For example, using the center loss (Wen et al. A discriminative feature learning approach for deep face recognition. ECCV16), triplet-center loss, triplet loss (computing class centers after embedding).  2. Discuss more on the outlier score part.  How to differentiate the known class outlier and new class? Will the problem be more difficult when the unknown class contains more heterogeneous classes? The authors can also apply existing open set recognition rule on the learned embedding.  Some detailed questions: 1. What's the difference between "the network weights are first updated to minimize on ii-loss and then in a separate step updated to minimize cross entropy loss" and optimize both loss terms simultaneously? 2. "We assume that a certain percent of the training set to be noise/outliers", how to determine the concrete value? Is 1% the helpful one for all cases? 3. Since there is not optimize over the unknown classes in training, could the reason for "the unknown class instances fully occupy the open space between the known classes" is the unknown classes are randomly sampled from the whole class set? For example, if classes about animals are known classes and classes about scene compose the unknown class, will the unknown class also occupy the whole space in this case? 4. What is the motivation of making "the unknown class instances fully occupy the open space between the known classes"?
Summary:  This paper proposes to extend the pretraining used for word representations in QA (e.g., ELMO) in the following sense: Instead of just predicting next/previous words in a sentence/paragraph, performing a hierarchical prediction over the whole document, by having a local LSTM and a global LSTM as presented in Fig. 1 + the idea of masked language model. Authors show meaningful improvements in 3 tasks that require document level understanding: extractive summarization, document segmentation, and answer passage retrieval for doc level QA.   Pros: - Good presentation and clear explanations. - Meaningful improvements in various tasks requiring document level understanding.  Cons: - Novelty is mainly incremental  Minor comment:  - Use a bigger picture for Fig. 1 - In page 1, Introduction, paragraph 2, line 10, "due the long-distance ..." ==> "due to the long-distance ..."  ********** I would like to thank authors for their feedback. After reading their feedback I still believe that novelty is incremental and would like to keep my score.
In this work, the authors explore different ways to pre-train contextualized word and sentence representations for use in other tasks. They propose two main methods: a straight-forward extension of the ElMO model for hierarchical uni-directional language models, and a de-noising auto-encoder type method which allows to train bi-directional representations. The learned contextual representations are evaluated on three downstream tasks, demonstrating the superiority of the bi-directional training setting, and beating strong baselines on extractive summarization.  The method is clearly presented and easy to follow, and the experiments do seem to support the author's claims, but their exposition misses several important details (or could be presented more clearly). For the document segmentation task, are the articles taken from a held-out set, or are they contained in the pre-training set? For passage retrieval, is the representation the same or are the representations re-trained from scratch using paragraph blocks? What exactly are the other features (those can go in the appendix)? And for the extractive summarization task, how many sentences are selected? Is pre-training also done on Wikipedia, or are those representations trained on news text?  A comparison to non-contextualized sentence representations would also be welcome (SkipThought, InferSent, ElMO-pool for settings other than passage retrieval). Note also that the local pre-training is not equivalent to ElMO, as the later sees context form the whole document rather than just the current sentence.  It is interesting to see that contextualized sentence representations can be used and that the Mask-LM objective yields better results than L+R-LM, but these points would be better made if the above questions were answered.
The paper is written rather well, however I find the experiments incomplete and have some reservations about the method. My main points of critique are:  1.  Combining DT & NN  I have doubts that the way you combine DT &NN  you get the "Best of both world". In some ways your architecture also  shares disadvantages of both:  1.1 Interpretability Because each node in the tree can a neural network (with arbritrary complexity), this approach looses one central advantage of DT, that is the interpretability of the result.    Each node in the tree can perform arbritrary complex (and hierarchical)  computations. The authors only show one particular example (Fig. 2a), where the model has learned is a reasonable  structure.  1.2 Complexity: The whole architecture is much more complex than either a neural network or a decision tree. I expect that therefore training these is not easy, and expert knowledge in either DT  or NN may not be enough to use this model.   2. Limited experiments  2.1 The authors only consider 2 experiments from vision (MNIST & CIFAR 10) while proposing a universal method.  To show universality the authors should use data sets from different domains (e.g UCI data sets)  2.2 The authors argue that a  strength of the method   is  that it uses a low number of parameters on average for a forward path (compared to the total parameter size).  I don't find this argument to be convincing. In the limit this would imply a high memorization of the  data.  Also, a similar case can be made for standard CNN, when a particular filter is mostly inactive for some data points.  2.3 The interpretability of DT compared to NN I mentioned earlier.  To make the argument that their method learns the hierarchical structure of the data , the authors should have added experiments to support this, where  such a hierarchical structure is clearly present and can be evaluated empirically.   --  In light of the extended experiments w.r.t. to 2.1 I increased my score from 5 to 6.  Overall, I still have doubts about the interpretability and complexity of the proposed method.    Complexity:  "but all the intuitions needed would come solely from training NN".    I disagree with this response.   The architecture is a mix between a tree (hard, decision-tree like error surface,  non-local) and neural network (smooth, mostly convex error surface). This also implies that the training process and its behavior will possess patterns and challenges of both approaches.   Interpretability:  I think the method misses "priors" that enforce credit assignment.  Partitioning the problem in subp-roblems should be done via the tree components, whereas processing (such as image filtering) should be done in the network nodes. However,  the method does not enforce, or encourage this behavior, for instance via constraints:   also nodes can do partitioning (because neural networks can approximate decision trees)  and edges can do processing (e.g. decisions-trees can be used for mnist).  So I still believe this to be a borderline paper, however, the experiments support a more general applicability.
The authors proposed a new model Adaptive Neural Trees(ANTs) by combining the representation learning and gradient optimization of neural networks with architecture learning of decision trees. The key advantage of the new model ANTs  over the existing methods(Random forest, Linear classifier, Neural decision forest, et al) is: it may achieve high accuracy(above ) with relatively much smaller number of parameters, as shown by the experiments on the datasets MNIST and CIFAR-10. Besides, the authors proposed single-path inference based on the greedily-selected leaf node to approximate the multi-path inferences with the full predictive distribution. The experiments show the single-path inference doesn't lose much accuracy but it saves memory and time. This paper is acceptable after minor modification.   Questions: In the second line below equation (1),  in  is not defined. Also, should  be ?
[summary] This paper considers natural gradient learning in GAN learning, where the Riemannian structure induced by the Wasserstein-2 distance is employed. More concretely, the constrained Wasserstein-2 metric , the geodesic distance on the parameter space induced by the Wasserstein-2 distance in the ambient space, is introduced (Theorem 1). The natural gradient on the parameter space with respect to the constrained Wasserstein-2 metric is then derived (Theorem 2). Since direct evaluation of  poses difficulty, the authors go on to considering a backward scheme using the proximal operator (3), yielding: (i) The Semi-Backward Euler method is proposed via a second-order Taylor approximation of the proximal operator  (Proposition 3). (ii) From an alternative formulation for  (Proposition 4), the authors propose dropping the gradient constraint to define a relaxed Wasserstein metric , yielding a simple proximal operator given by the expected squared Euclidean distance in the sample space used as a regularizer (equation (4)). The resulting algorithm is termed the Relaxed Wasserstein Proximal (RWP) algorithm.  [pros] The proposal provides an easy-to-implement drop-in regularizer framework, so that it can straightforwardly be combined with various generator update schemes.  [cons] Despite all the theoretical arguments given to justify the proposal, the resulting proposal may simply be viewed as a naive application of the proximal operator.  [Quality] See [Detailed comments] section below.  [Clarity] This paper is basically clearly written.  [Originality] Providing justification to the proximal operator approach in GAN learning via natural gradient with respect to the Riemannian structure seems original.  [Significance] See [Detailed comments] section below.  [Detailed comments] To the parameter space , one can consider introducing several different Riemannian structures, including the conventional Euclidean structure and that induced by the Wasserstein-2 metric. Which Riemannian structure among all these possibilities would be natural and efficient in GAN training would not be evident, and this paper discusses this issue only in the very special single instance in Section 2.3. A more thorough argument supporting superiority of the Riemannian structure induced by the Wasserstein-2 metric would thus be needed in order to justify the proposed approach.  In relation to this, the result of comparison between WGAN-GP with and without SBE shown in Figure 5 is embarrassing to me, since it might suggest that the proposed framework aiming at performing Wasserstein natural gradient is not so efficient if combined with WGAN-GP. The natural gradient is expected to be efficient when the underlying coordinate system is non-orthonormal (Amari, 1998). Starting with the gradient descent iteration derived from the backward Euler method in (3), which is computationally hard, the argument in this paper goes on to propose two methods: the Semi-Backward Euler method via a second-order Taylor approximation to the backward Euler scheme (Proposition 3), and RWP in (4) via approximation (dropping of the gradient constraint and finite-difference approximation in the integral with respect to ) of an alternative simpler formulation for the Wasserstein metric (Proposition 4). These two methods involve different approximations to the Semi-Backward Euler, and one would like to know why the approximations in the latter method is better in performance than those in the former. Discussion on this point is however missing in this paper.  In Section 3, it would have been better if the performance be compared not only in terms of FID but also the loss considered (i.e., Wasserstein-1), since the latter is exactly what the algorithms are trying to optimize.  Minor points:  Page 4: The line just after equation (4) should be moved to the position following the equation giving .  In the reference list, the NIPS paper by Gulrajani et al. appears twice.
